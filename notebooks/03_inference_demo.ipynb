{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6da99e",
   "metadata": {},
   "source": [
    "# üîÆ ThermoSight: Inference Demo & Visualization\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "- **Load a trained ThermoSight model** üß†\n",
    "- **Perform inference on new microscope images** (single and batch) üñºÔ∏è\n",
    "- **Visualize prediction probabilities** üìä\n",
    "- **Display results clearly** ‚ú®\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "from src.models.vit_model import ViT # Assuming ViT model is defined here\n",
    "from src.inference.predict import predict_image # Using the standalone predict_image\n",
    "\n",
    "# Setup\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"üî¨ Inference Demo Initialized!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class InferenceConfig:\n",
    "    model_path = os.path.join('..', 'models', 'best_model.pth')\n",
    "    # Example image directory (ensure this path is correct and contains images)\n",
    "    sample_image_dir = os.path.join('..', 'data', 'processed', 'test') \n",
    "    \n",
    "    img_size = 460 # Must match training\n",
    "    patch_size = 8 # Must match training\n",
    "    num_classes = 4 # Must match training\n",
    "    \n",
    "    # Class names (ensure order matches model output)\n",
    "    class_names = ['200¬∞C', '400¬∞C', '600¬∞C', '800¬∞C'] \n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "print(f\"‚öôÔ∏è Configuration Loaded:\")\n",
    "print(f\"  Model path: {config.model_path}\")\n",
    "print(f\"  Sample image directory: {config.sample_image_dir}\")\n",
    "print(f\"  Device: {config.device}\")\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(config.model_path):\n",
    "    print(f\"üö® WARNING: Model file not found at {config.model_path}\")\n",
    "    print(\"üí° Please ensure a trained model ('best_model.pth') is in the '../models/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Trained Model\n",
    "def load_model(model_path, img_size, patch_size, num_classes, device):\n",
    "    \"\"\"Loads the trained ViT model.\"\"\"\n",
    "    try:\n",
    "        model = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_classes=num_classes,\n",
    "            # Add other ViT params if they differ from defaults used in src.models.vit_model\n",
    "            embed_dim=768, # Example, ensure these match your trained model\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            mlp_ratio=4.0,\n",
    "            dropout=0.0 # Typically set dropout to 0 for eval\n",
    "        )\n",
    "        # Load state dict\n",
    "        # Ensure the map_location is correctly set, especially if trained on GPU and inferring on CPU\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Handle potential DataParallel prefix if model was saved that way\n",
    "        if next(iter(state_dict)).startswith('module.'):\n",
    "            state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "            \n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"‚úÖ Model loaded successfully from {model_path} and set to evaluation mode.\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Model file not found at {model_path}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the model\n",
    "model = load_model(config.model_path, config.img_size, config.patch_size, config.num_classes, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6fc558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "def preprocess_image(image_path, img_size):\n",
    "    \"\"\"Preprocesses a single image for inference.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Define the same transformations used during validation/testing\n",
    "        preprocess_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)), # Or Resize(int(img_size * 256 / 224)) then CenterCrop(img_size)\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        img_tensor = preprocess_transform(img)\n",
    "        return img_tensor.unsqueeze(0) # Add batch dimension\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Image file not found at {image_path}.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error preprocessing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Select a sample image\n",
    "if os.path.exists(config.sample_image_dir):\n",
    "    try:\n",
    "        # Get a random class and a random image from that class\n",
    "        random_class = random.choice([d for d in os.listdir(config.sample_image_dir) if os.path.isdir(os.path.join(config.sample_image_dir, d))])\n",
    "        class_path = os.path.join(config.sample_image_dir, random_class)\n",
    "        image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp'))]\n",
    "        if image_files:\n",
    "            sample_image_name = random.choice(image_files)\n",
    "            sample_image_path = os.path.join(class_path, sample_image_name)\n",
    "            print(f\"üñºÔ∏è Sample image selected: {sample_image_path}\")\n",
    "\n",
    "            # Preprocess the sample image\n",
    "            input_tensor = preprocess_image(sample_image_path, config.img_size)\n",
    "            if input_tensor is not None:\n",
    "                print(f\"Processed tensor shape: {input_tensor.shape}\")\n",
    "        else:\n",
    "            print(f\"üö® No images found in {class_path}\")\n",
    "            sample_image_path = None\n",
    "            input_tensor = None\n",
    "    except IndexError:\n",
    "        print(f\"üö® No subdirectories (classes) found in {config.sample_image_dir}\")\n",
    "        sample_image_path = None\n",
    "        input_tensor = None\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Error selecting sample image: {e}\")\n",
    "        sample_image_path = None\n",
    "        input_tensor = None\n",
    "else:\n",
    "    print(f\"üö® Sample image directory not found: {config.sample_image_dir}\")\n",
    "    sample_image_path = None\n",
    "    input_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbd8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Image Inference\n",
    "def predict_single(model, image_tensor, device, class_names):\n",
    "    \"\"\"Performs inference on a single preprocessed image tensor.\"\"\"\n",
    "    if model is None or image_tensor is None:\n",
    "        print(\"Model or image tensor is not available for prediction.\")\n",
    "        return None, None\n",
    "        \n",
    "    with torch.no_grad(): # Ensure no gradients are computed\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        logits = model(image_tensor)\n",
    "        probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "        predicted_class_idx = np.argmax(probabilities)\n",
    "        predicted_class_name = class_names[predicted_class_idx]\n",
    "        confidence = probabilities[predicted_class_idx]\n",
    "        \n",
    "    return predicted_class_name, confidence, probabilities\n",
    "\n",
    "# Run prediction on the sample image\n",
    "if model and input_tensor is not None:\n",
    "    predicted_class, confidence, probs = predict_single(model, input_tensor, config.device, config.class_names)\n",
    "    \n",
    "    if predicted_class:\n",
    "        print(f\"\\nüéØ Prediction for: {os.path.basename(sample_image_path) if sample_image_path else 'N/A'}\")\n",
    "        print(f\"  Predicted Class: {predicted_class}\")\n",
    "        print(f\"  Confidence: {confidence:.4f}\")\n",
    "        \n",
    "        # Display image and probabilities\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Display image\n",
    "        if sample_image_path:\n",
    "            img_display = Image.open(sample_image_path)\n",
    "            axes[0].imshow(img_display)\n",
    "        axes[0].set_title(f\"Input Image\\nTrue Class: {random_class if sample_image_path else 'N/A'}\", fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Display probabilities\n",
    "        bars = sns.barplot(x=config.class_names, y=probs, ax=axes[1], palette=\"viridis\")\n",
    "        axes[1].set_title(f\"Prediction Probabilities\\nPredicted: {predicted_class} ({confidence:.2f})\", fontsize=12)\n",
    "        axes[1].set_ylabel(\"Probability\", fontsize=10)\n",
    "        axes[1].set_xlabel(\"Temperature Class\", fontsize=10)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "        axes[1].set_ylim(0, 1)\n",
    "\n",
    "        # Add probability values on bars\n",
    "        for i, bar in enumerate(bars.patches):\n",
    "            axes[1].text(bar.get_x() + bar.get_width() / 2,\n",
    "                         bar.get_height() + 0.02,\n",
    "                         f'{probs[i]:.2f}',\n",
    "                         ha='center', va='bottom', fontsize=9)\n",
    "            if i == np.argmax(probs): # Highlight predicted class\n",
    "                bar.set_color('orangered')\n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping single image inference as model or input tensor is missing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8ec67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Inference and Visualization\n",
    "def predict_batch(model, image_paths, img_size, device, class_names):\n",
    "    \"\"\"Performs inference on a batch of image paths.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model is not available for batch prediction.\")\n",
    "        return []\n",
    "        \n",
    "    results = []\n",
    "    for img_path in image_paths:\n",
    "        input_tensor = preprocess_image(img_path, img_size)\n",
    "        if input_tensor is not None:\n",
    "            pred_class, conf, _ = predict_single(model, input_tensor, device, class_names)\n",
    "            if pred_class:\n",
    "                results.append({\n",
    "                    'path': img_path,\n",
    "                    'true_class': os.path.basename(os.path.dirname(img_path)), # Assuming parent folder is class name\n",
    "                    'predicted_class': pred_class,\n",
    "                    'confidence': conf\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# Select a few random images for batch prediction\n",
    "batch_image_paths = []\n",
    "if os.path.exists(config.sample_image_dir):\n",
    "    all_image_files = []\n",
    "    for cls_name in os.listdir(config.sample_image_dir):\n",
    "        cls_dir_path = os.path.join(config.sample_image_dir, cls_name)\n",
    "        if os.path.isdir(cls_dir_path):\n",
    "            for fname in os.listdir(cls_dir_path):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "                    all_image_files.append(os.path.join(cls_dir_path, fname))\n",
    "    \n",
    "    if len(all_image_files) > 0:\n",
    "        num_batch_samples = min(len(all_image_files), 4) # Predict on 4 images\n",
    "        batch_image_paths = random.sample(all_image_files, num_batch_samples)\n",
    "        print(f\"\\nSelected {num_batch_samples} images for batch prediction.\")\n",
    "    else:\n",
    "        print(\"No images found for batch prediction.\")\n",
    "\n",
    "if model and batch_image_paths:\n",
    "    batch_results = predict_batch(model, batch_image_paths, config.img_size, config.device, config.class_names)\n",
    "    \n",
    "    print(\"\\nüì¶ Batch Prediction Results:\")\n",
    "    for res in batch_results:\n",
    "        print(f\"  Image: {os.path.basename(res['path'])} | True: {res['true_class']} | Predicted: {res['predicted_class']} | Confidence: {res['confidence']:.3f}\")\n",
    "\n",
    "    # Visualize batch predictions\n",
    "    if batch_results:\n",
    "        num_images = len(batch_results)\n",
    "        fig, axes = plt.subplots(num_images, 1, figsize=(8, 3 * num_images))\n",
    "        if num_images == 1: axes = [axes] # Ensure axes is iterable for single image\n",
    "\n",
    "        for i, res in enumerate(batch_results):\n",
    "            img = Image.open(res['path'])\n",
    "            axes[i].imshow(img)\n",
    "            title = (f\"True: {res['true_class']}\\n\"\n",
    "                     f\"Predicted: {res['predicted_class']} (Conf: {res['confidence']:.2f})\")\n",
    "            axes[i].set_title(title, fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping batch inference as model or image paths are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876ec9a",
   "metadata": {},
   "source": [
    "## ü§î Advanced: Visualizing Model Attention (e.g., Grad-CAM)\n",
    "\n",
    "To understand *why* the model makes certain predictions, techniques like Grad-CAM can be used to highlight the regions of the image that were most influential. Implementing Grad-CAM for Vision Transformers is more involved than for CNNs but is possible.\n",
    "\n",
    "**Conceptual Steps for ViT Grad-CAM:**\n",
    "1.  Hook into the last attention layer or a specific block.\n",
    "2.  Compute gradients of the predicted class score with respect to the attention map outputs.\n",
    "3.  Weight the attention maps using these gradients.\n",
    "4.  Average and upscale the resulting heatmap to overlay on the input image.\n",
    "\n",
    "This typically requires a library that supports ViT attention visualization or custom implementation. For brevity, it's not fully implemented here but is a valuable next step for model interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Inference Demo Complete!** You can now use the `predict_single` or `predict_batch` functions with your own images."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
