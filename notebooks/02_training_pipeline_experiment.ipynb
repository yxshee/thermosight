{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d815ef6",
   "metadata": {},
   "source": [
    "# üöÄ ThermoSight: Training Pipeline Experiment\n",
    "\n",
    "This notebook implements and monitors the complete training pipeline for ThermoSight:\n",
    "- **Configuration Setup**: Define all hyperparameters and paths.\n",
    "- **Advanced Data Loading**: Includes augmentations and efficient batching.\n",
    "- **Model Architecture**: Define and initialize the Vision Transformer (ViT).\n",
    "- **Training Loop**: With detailed progress, loss, and learning rate tracking.\n",
    "- **Evaluation**: Calculate accuracy, confusion matrix, and per-class metrics.\n",
    "- **TensorBoard Logging**: For real-time monitoring of metrics and visualizations.\n",
    "- **Model Checkpointing**: Save the best performing model.\n",
    "- **Results Visualization**: Plot training curves.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c53ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Global Setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix as sk_confusion_matrix\n",
    "\n",
    "# Add src to path for custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join('..'))) # Go up one level to ThermoSight root\n",
    "from src.models.vit_model import ViT\n",
    "from src.utils.visualize import plot_metrics as tb_plot_metrics # Renamed to avoid conflict\n",
    "from src.utils.visualize import plot_confusion_matrix as tb_plot_confusion_matrix # Renamed\n",
    "from src.evaluate.metrics import per_class_accuracy\n",
    "\n",
    "# Notebook specific settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b09dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Class\n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Data paths\n",
    "        self.base_dir = os.path.join('..') # Project root\n",
    "        self.data_dir = os.path.join(self.base_dir, 'data', 'processed')\n",
    "        self.train_dir = os.path.join(self.data_dir, 'train')\n",
    "        self.test_dir = os.path.join(self.data_dir, 'test')\n",
    "\n",
    "        # Model parameters\n",
    "        self.img_size = 460\n",
    "        self.patch_size = 8\n",
    "        self.in_channels = 3\n",
    "        self.num_classes = 4 # Adjust if your dataset has different number of classes\n",
    "        self.embed_dim = 768\n",
    "        self.depth = 12\n",
    "        self.num_heads = 12\n",
    "        self.mlp_ratio = 4.0\n",
    "        self.dropout = 0.1 # Dropout for training\n",
    "\n",
    "        # Training hyperparameters\n",
    "        self.batch_size = 16 # Adjust based on GPU memory\n",
    "        self.learning_rate = 3e-5 # ViTs often benefit from smaller LRs\n",
    "        self.weight_decay = 0.05\n",
    "        self.num_epochs = 15 # Increase for full training, keep low for demo\n",
    "        self.warmup_epochs = 2\n",
    "        self.label_smoothing = 0.1\n",
    "\n",
    "        # Logging and saving\n",
    "        self.experiment_name = f\"ViT_exp_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.log_dir = os.path.join(self.base_dir, 'outputs', 'logs', self.experiment_name)\n",
    "        self.model_save_dir = os.path.join(self.base_dir, 'models')\n",
    "        self.best_model_name = f\"{self.experiment_name}_best_model.pth\"\n",
    "        \n",
    "        # Device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Ensure directories exist\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.model_save_dir, exist_ok=True)\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "print(\"Configuration Loaded:\")\n",
    "for key, value in config.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Check if data directories exist\n",
    "if not os.path.exists(config.train_dir) or not os.path.exists(config.test_dir):\n",
    "    print(f\"üö® WARNING: Data directories not found at {config.train_dir} or {config.test_dir}\")\n",
    "    print(\"üí° Please run the data preparation script (e.g., src/data/make_dataset.py) first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms and Loaders\n",
    "def get_data_transforms(img_size):\n",
    "    # Normalization parameters (commonly ImageNet)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    \n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.Resize(img_size + 32), # Resize to a bit larger then center crop\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return train_transforms, test_transforms\n",
    "\n",
    "train_tf, test_tf = get_data_transforms(config.img_size)\n",
    "\n",
    "# Create datasets\n",
    "try:\n",
    "    train_dataset = datasets.ImageFolder(config.train_dir, transform=train_tf)\n",
    "    test_dataset = datasets.ImageFolder(config.test_dir, transform=test_tf)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    print(f\"üñºÔ∏è Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "    print(f\"üè∑Ô∏è Classes: {train_dataset.classes}\")\n",
    "    config.num_classes = len(train_dataset.classes) # Update num_classes from dataset\n",
    "    print(f\"Updated num_classes to: {config.num_classes}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: Data directory not found. {e}\")\n",
    "    print(\"üí° Make sure 'data/processed/train' and 'data/processed/test' exist and contain class subfolders.\")\n",
    "    train_loader, test_loader = None, None # Set to None to prevent further execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, Optimizer, Scheduler\n",
    "if train_loader and test_loader: # Proceed only if data loaded\n",
    "    model = ViT(\n",
    "        img_size=config.img_size,\n",
    "        patch_size=config.patch_size,\n",
    "        in_channels=config.in_channels,\n",
    "        num_classes=config.num_classes,\n",
    "        embed_dim=config.embed_dim,\n",
    "        depth=config.depth,\n",
    "        num_heads=config.num_heads,\n",
    "        mlp_ratio=config.mlp_ratio,\n",
    "        dropout=config.dropout\n",
    "    ).to(config.device)\n",
    "\n",
    "    print(f\"ü§ñ Model Initialized: {model.__class__.__name__}\")\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   Trainable Parameters: {num_params/1e6:.2f}M\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    \n",
    "    # Scheduler: OneCycleLR\n",
    "    total_steps = len(train_loader) * config.num_epochs\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=config.learning_rate, \n",
    "        total_steps=total_steps,\n",
    "        pct_start=config.warmup_epochs/config.num_epochs if config.num_epochs > 0 else 0.1\n",
    "    )\n",
    "\n",
    "    # TensorBoard Writer\n",
    "    writer = SummaryWriter(log_dir=config.log_dir)\n",
    "    print(f\"üìä TensorBoard logs will be saved to: {config.log_dir}\")\n",
    "    # Log hyperparameters\n",
    "    writer.add_hparams(\n",
    "        {k: v for k, v in config.__dict__.items() if isinstance(v, (int, float, str, bool))},\n",
    "        {} # No metrics to log at hparam definition time\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping model and optimizer setup due to data loading issues.\")\n",
    "    model, writer = None, None # Ensure these are None if data loading failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758769f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Loop\n",
    "def train_one_epoch(model, loader, criterion, optimizer, scheduler, device, epoch, writer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{config.num_epochs} [Training]\", leave=False)\n",
    "    for i, (images, labels) in enumerate(progress_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # Step scheduler at each batch for OneCycleLR\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels.data)\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Log batch loss and LR to TensorBoard\n",
    "        current_iter = epoch * len(loader) + i\n",
    "        writer.add_scalar('Loss/train_batch', loss.item(), current_iter)\n",
    "        writer.add_scalar('LR/batch', scheduler.get_last_lr()[0], current_iter)\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item(), lr=f\"{scheduler.get_last_lr()[0]:.1e}\")\n",
    "        \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    return epoch_loss, epoch_acc.item()\n",
    "\n",
    "def evaluate(model, loader, criterion, device, epoch, writer, class_names):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{config.num_epochs} [Evaluating]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions.double() / total_samples\n",
    "    \n",
    "    # Confusion Matrix and Per-Class Accuracy\n",
    "    cm = sk_confusion_matrix(all_labels, all_preds, labels=list(range(config.num_classes)))\n",
    "    pca = per_class_accuracy(cm)\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    tb_plot_metrics(writer, epoch_loss, epoch_acc.item(), epoch) # Using renamed util\n",
    "    tb_plot_confusion_matrix(writer, cm, epoch, class_names=class_names) # Using renamed util\n",
    "    for i, acc_val in enumerate(pca):\n",
    "        writer.add_scalar(f'Accuracy/class_{class_names[i]}', acc_val, epoch)\n",
    "        \n",
    "    return epoch_loss, epoch_acc.item(), cm, pca\n",
    "\n",
    "# --- Training Execution ---\n",
    "if model and train_loader and test_loader and writer: # Check if setup was successful\n",
    "    best_test_acc = 0.0\n",
    "    history = defaultdict(list)\n",
    "    class_names = train_dataset.classes\n",
    "\n",
    "    print(f\"\\nüöÄ Starting Training for {config.num_epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, scheduler, config.device, epoch, writer)\n",
    "        test_loss, test_acc, cm, pca = evaluate(model, test_loader, criterion, config.device, epoch, writer, class_names)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['lr'].append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{config.num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f} | \"\n",
    "              f\"LR: {scheduler.get_last_lr()[0]:.1e}\")\n",
    "\n",
    "        # Save best model\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            model_save_path = os.path.join(config.model_save_dir, config.best_model_name)\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"‚ú® New best model saved to {model_save_path} (Test Acc: {best_test_acc:.4f})\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nüèÅ Training Finished! Total time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"üèÜ Best Test Accuracy: {best_test_acc:.4f}\")\n",
    "    \n",
    "    # Log final metrics to hparams\n",
    "    writer.add_hparams(\n",
    "        {k: v for k, v in config.__dict__.items() if isinstance(v, (int, float, str, bool))},\n",
    "        {\"hparam/best_test_accuracy\": best_test_acc,\n",
    "         \"hparam/final_train_accuracy\": train_acc,\n",
    "         \"hparam/final_test_accuracy\": test_acc}\n",
    "    )\n",
    "    writer.close()\n",
    "else:\n",
    "    print(\"‚ùå Training aborted due to setup issues (model, data, or writer not initialized).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb931686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training History\n",
    "if model and 'train_loss' in history and len(history['train_loss']) > 0: # Check if training ran\n",
    "    epochs_range = range(1, config.num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range, history['train_loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(epochs_range, history['test_loss'], label='Test Loss', marker='x')\n",
    "    plt.title('Loss vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range, history['train_acc'], label='Train Accuracy', marker='o')\n",
    "    plt.plot(epochs_range, history['test_acc'], label='Test Accuracy', marker='x')\n",
    "    plt.title('Accuracy vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Learning Rate\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs_range, history['lr'], label='Learning Rate', color='green', marker='.')\n",
    "    plt.title('Learning Rate vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log') # Often better to view LR on log scale\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display final confusion matrix from the last epoch\n",
    "    if 'cm' in locals() and cm is not None: # Check if cm was computed\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title(f'Confusion Matrix (Epoch {config.num_epochs})')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nPer-class Accuracy (Epoch {config.num_epochs}):\")\n",
    "        if 'pca' in locals() and pca is not None:\n",
    "            for i, acc_val in enumerate(pca):\n",
    "                print(f\"  {class_names[i]}: {acc_val:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5498a3",
   "metadata": {},
   "source": [
    "## üí° Next Steps & Experimentation Ideas\n",
    "\n",
    "- **Hyperparameter Tuning**:\n",
    "    - Experiment with different learning rates, weight decay, batch sizes.\n",
    "    - Try different optimizer (e.g., SGD with momentum) or schedulers.\n",
    "    - Adjust `mlp_ratio`, `embed_dim`, `depth`, `num_heads` in ViT.\n",
    "- **Data Augmentation**:\n",
    "    - Explore more advanced augmentation techniques (e.g., Mixup, CutMix).\n",
    "    - Analyze the impact of different augmentation strengths.\n",
    "- **Regularization**:\n",
    "    - Experiment with different dropout rates or other regularization methods like Stochastic Depth.\n",
    "- **Transfer Learning**:\n",
    "    - Initialize ViT with pre-trained weights (e.g., from ImageNet) and fine-tune.\n",
    "- **Longer Training**:\n",
    "    - Run for more epochs to see if performance improves further.\n",
    "- **Advanced Logging**:\n",
    "    - Log model gradients or activation histograms to TensorBoard for deeper insights.\n",
    "- **Cross-Validation**:\n",
    "    - Implement k-fold cross-validation for more robust evaluation if dataset size permits.\n",
    "\n",
    "---\n",
    "\n",
    "üöÄ **Experimentation is key to achieving optimal performance!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
